{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28056\n",
      "28056\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load the tokens from pickle files\n",
    "loc_tokens = pkl.load(open(\"loc_tokens.p\", \"rb\"))\n",
    "\n",
    "loc_label = pd.read_csv('location.csv', sep='\\t')['Y']\n",
    "print(len(loc_tokens))\n",
    "print(len(loc_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 11882, 1: 6988, 2: 2168})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "x = []\n",
    "y = []\n",
    "data = loc_tokens\n",
    "label = loc_label.tolist()\n",
    "for i in range(len(data)):\n",
    "    if len(data[i]) != 0 and float(label[i]) >= 0:\n",
    "        x.append(data[i])\n",
    "        y.append(int(label[i]))\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=99)\n",
    "Counter(y_train)\n",
    "max_vocab_size = 50000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "for token in x_train:\n",
    "    all_tokens += token\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "token2id, id2token = build_vocab(all_tokens)\n",
    "\n",
    "    \n",
    "train_data_indices = token2index_dataset(x_train)\n",
    "val_data_indices = token2index_dataset(x_test)\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('token2id.json', 'w') as fp:\n",
    "    json.dump(token2id, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('id2token.txt', 'w') as filehandle:  \n",
    "    for listitem in id2token:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LocDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_list, target_list):\n",
    "  \n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def loc_collate_func(batch):\n",
    "\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list)).to(device), \n",
    "            torch.LongTensor(length_list).to(device), torch.LongTensor(label_list).to(device)]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = LocDataset(train_data_indices, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=loc_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = LocDataset(val_data_indices, y_test)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=loc_collate_func,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BagOfWords(\n",
       "  (embed): Embedding(39750, 100, padding_idx=0)\n",
       "  (relu): ReLU()\n",
       "  (linear): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "   \n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(emb_dim,3)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "#         parameters = self.embed.weight.cpu().detach().numpy()\n",
    "        # return logits\n",
    "        out = self.relu(out.float())\n",
    "        out = self.linear(out.float())\n",
    "        \n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/1315], Validation Acc: 56.83730215314416\n",
      "Epoch: [1/3], Step: [201/1315], Validation Acc: 56.83730215314416\n",
      "Epoch: [1/3], Step: [301/1315], Validation Acc: 56.83730215314416\n",
      "Epoch: [1/3], Step: [401/1315], Validation Acc: 56.82304292029089\n",
      "Epoch: [1/3], Step: [501/1315], Validation Acc: 56.83730215314416\n",
      "Epoch: [1/3], Step: [601/1315], Validation Acc: 56.865820618850705\n",
      "Epoch: [1/3], Step: [701/1315], Validation Acc: 57.293597604448884\n",
      "Epoch: [1/3], Step: [801/1315], Validation Acc: 57.62155996007415\n",
      "Epoch: [1/3], Step: [901/1315], Validation Acc: 57.50748609724797\n",
      "Epoch: [1/3], Step: [1001/1315], Validation Acc: 57.7498930557536\n",
      "Epoch: [1/3], Step: [1101/1315], Validation Acc: 58.99044631398831\n",
      "Epoch: [1/3], Step: [1201/1315], Validation Acc: 59.018964779694855\n",
      "Epoch: [1/3], Step: [1301/1315], Validation Acc: 59.38970483387994\n",
      "Epoch: [2/3], Step: [101/1315], Validation Acc: 60.25951803792956\n",
      "Epoch: [2/3], Step: [201/1315], Validation Acc: 60.188221873663196\n",
      "Epoch: [2/3], Step: [301/1315], Validation Acc: 60.15970340795665\n",
      "Epoch: [2/3], Step: [401/1315], Validation Acc: 60.644517324967914\n",
      "Epoch: [2/3], Step: [501/1315], Validation Acc: 60.94396121488664\n",
      "Epoch: [2/3], Step: [601/1315], Validation Acc: 60.87266505062028\n",
      "Epoch: [2/3], Step: [701/1315], Validation Acc: 60.915442749180094\n",
      "Epoch: [2/3], Step: [801/1315], Validation Acc: 61.172108940539\n",
      "Epoch: [2/3], Step: [901/1315], Validation Acc: 61.214886639098815\n",
      "Epoch: [2/3], Step: [1001/1315], Validation Acc: 61.40025666619136\n",
      "Epoch: [2/3], Step: [1101/1315], Validation Acc: 61.628404391843716\n",
      "Epoch: [2/3], Step: [1201/1315], Validation Acc: 61.64266362469699\n",
      "Epoch: [2/3], Step: [1301/1315], Validation Acc: 61.314701269071726\n",
      "Epoch: [3/3], Step: [101/1315], Validation Acc: 61.54284899472408\n",
      "Epoch: [3/3], Step: [201/1315], Validation Acc: 61.71395978896335\n",
      "Epoch: [3/3], Step: [301/1315], Validation Acc: 61.52858976187081\n",
      "Epoch: [3/3], Step: [401/1315], Validation Acc: 61.614145158990446\n",
      "Epoch: [3/3], Step: [501/1315], Validation Acc: 61.57136746043063\n",
      "Epoch: [3/3], Step: [601/1315], Validation Acc: 61.85655211749608\n",
      "Epoch: [3/3], Step: [701/1315], Validation Acc: 61.81377441893626\n",
      "Epoch: [3/3], Step: [801/1315], Validation Acc: 61.51433052901754\n",
      "Epoch: [3/3], Step: [901/1315], Validation Acc: 61.65692285755026\n",
      "Epoch: [3/3], Step: [1001/1315], Validation Acc: 61.7424782546699\n",
      "Epoch: [3/3], Step: [1101/1315], Validation Acc: 61.68544132325681\n",
      "Epoch: [3/3], Step: [1201/1315], Validation Acc: 61.67118209040353\n",
      "Epoch: [3/3], Step: [1301/1315], Validation Acc: 62.084699843148435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3 # number epoch to train\n",
    "ngram = 1\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        data_batch = data_batch.long()\n",
    "        out = model(data_batch, length_batch)\n",
    "        outputs = F.softmax(out, dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "#         print(Counter(predicted.to('cpu').numpy().ravel().tolist()))\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        data_batch = data_batch.long()\n",
    "        label_batch = label_batch.long()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('embedding.txt',model.embed.weight.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
